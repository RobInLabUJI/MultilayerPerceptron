{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdYOvaIlVmvAwQpL99vRuM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RobInLabUJI/MultilayerPerceptron/blob/main/Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification example\n",
        "\n",
        "The main limitation of perceptrons was that they only worked with linearly separable classes.\n",
        "\n",
        "A multilayer perceptron (MLP) outperforms the linear perceptron and is able to solve linearly non-separable problems.\n",
        "\n",
        "In this notebook you will learn how to apply the MLP to a typical [classification task](http://scikit-learn.org/stable/modules/neural_networks_supervised.html#classification). The diagram shown below is a typical workflow diagram for such a task.\n",
        "\n",
        "![](https://github.com/RobInLabUJI/MultilayerPerceptron/raw/main/img/FeatureExtraction.png)\n",
        "\n",
        "**1. Preprocessing - getting data into shape**\n",
        "\n",
        "Raw data rarely comes in the form and shape that is necessary for the optimal performance of an algorithm. The first steps are the preprocessing tasks: loading the data, scaling it, and splitting the data into training and testing sets.\n",
        "\n",
        "**2. Training and selecting a predictive model**\n",
        "\n",
        "In our case the predictive model is the MLP, and its parameters need to be set before starting to train.\n",
        "\n",
        "**3. Evaluating models and predicting unseen data instances**\n",
        "\n",
        "After we have selected a model that has been fitted on the training data set, we can use the test data set to estimate how well it performs on this unseen data to estimate the generalization error.\n",
        "\n",
        "*The picture and the description of the process are from [\"Python Machine Learning by Sebastian Raschka, 2015\"](https://github.com/rasbt/python-machine-learning-book)*"
      ],
      "metadata": {
        "id": "U7NDNDcoJ1Qy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GO0j2rirJxxQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.datasets import make_moons, make_circles\n",
        "import joblib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "def plot_ds(X_train, X_test, y_train, y_test):\n",
        "    h = .02\n",
        "    x_min, x_max = X_train[:, 0].min() - .5, X_train[:, 0].max() + .5\n",
        "    y_min, y_max = X_train[:, 1].min() - .5, X_train[:, 1].max() + .5\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
        "    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright);\n",
        "    plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6);\n",
        "    plt.xlim(xx.min(), xx.max()); plt.ylim(yy.min(), yy.max());\n",
        "    plt.xticks(()); plt.yticks(());\n",
        "    \n",
        "def plot_contour(net,X_train, X_test, y_train, y_test):\n",
        "    h = .02\n",
        "    x_min, x_max = X_train[:, 0].min() - .5, X_train[:, 0].max() + .5\n",
        "    y_min, y_max = X_train[:, 1].min() - .5, X_train[:, 1].max() + .5\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    Z = net.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    cm = plt.cm.RdBu\n",
        "    plt.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load in the data\n",
        "\n",
        "scikit-learn includes various [random sample generators](http://scikit-learn.org/stable/datasets/#sample-generators) that can be used to build artificial datasets of controlled size and complexity. Choose one of these datasets by running one of the two code cells below:\n",
        "\n",
        "<table border=\"0\">\n",
        "<tr>\n",
        "<th>Moons</th>\n",
        "<th>Circles</th>\n",
        "</tr>\n",
        "<tr><td>\n",
        "<img src=\"https://github.com/RobInLabUJI/MultilayerPerceptron/raw/main/img/ds_moons.png\">\n",
        "</td><td>\n",
        "<img src=\"https://github.com/RobInLabUJI/MultilayerPerceptron/raw/main/img/ds_circles.png\">\n",
        "</td></tr>\n",
        "</table>"
      ],
      "metadata": {
        "id": "ohkte99vKliM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell for using dataset \"Moons\"\n",
        "ds = make_moons(n_samples=250, noise=0.3)"
      ],
      "metadata": {
        "id": "rTeCSvT4KTVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell for using dataset \"Circles\"\n",
        "ds = make_circles(n_samples=250, noise=0.2, factor=0.5)"
      ],
      "metadata": {
        "id": "mlUS_xxWKg1H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross-validation: evaluating network performance\n",
        "\n",
        "Learning the parameters of the MLP and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called [**overfitting**](https://en.wikipedia.org/wiki/Overfitting). To avoid it, it is common practice  to hold out part of the available data as a test set `[X_test, y_test]`. \n",
        "\n",
        "In scikit-learn a random split into training and test sets can be quickly computed with the [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) helper function."
      ],
      "metadata": {
        "id": "T8L-9Q-_Krnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = ds\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)"
      ],
      "metadata": {
        "id": "iHkH7dwQKqT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data scaling\n",
        "\n",
        "Standardization of datasets is a common requirement for neural networks; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance.\n",
        "\n",
        "In scikit-learn, the preprocessing module provides a utility class [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) that computes the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set."
      ],
      "metadata": {
        "id": "eZLfZ4T1Kvng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't cheat - fit only on training data\n",
        "scaler = StandardScaler().fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "# apply same transformation to test data\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "Vrsrt-9GKttO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_ds(X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "MuO2UqdGKyG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the model\n",
        "\n",
        "Create a [MLP object](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) with the following arguments:\n",
        "* ‘lbfgs’ solver, which is an optimizer in the family of quasi-Newton methods; for small datasets, it can converge fast and perform pretty well\n",
        "* one hidden layer with 5 neurons\n",
        "* 4000 iterations maximum\n",
        "\n",
        "The rest of the arguments are set to their default values (see documentation)."
      ],
      "metadata": {
        "id": "od5gWGVWK_f4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = MLPClassifier(solver='lbfgs',\\\n",
        "                    hidden_layer_sizes=(5,),\\\n",
        "                    max_iter=4000)"
      ],
      "metadata": {
        "id": "zVZntNzlKzWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the network\n",
        "\n",
        "The `fit` function automatically iterates until convergence or the maximum number of iterations is reached, so you only need to execute the following cell once."
      ],
      "metadata": {
        "id": "G2cBSF5rLEd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "Dbo28QW9LDZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot decision boundary\n",
        "\n",
        "Plot the decision boundary as a contour plot. For that, we will assign a color to each point in the plane, which will be proportional to its probability of belonging to one class or the other."
      ],
      "metadata": {
        "id": "GeBA6UFvLHyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_contour(net, X_train, X_test, y_train, y_test)\n",
        "plot_ds(X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "7tu1AJYeLGTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis of the network\n",
        "\n",
        "Percentage of correct classification of the test data:"
      ],
      "metadata": {
        "id": "sqAV8jCpLNkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Score: %.2f' % (net.score(X_test, y_test)*100))"
      ],
      "metadata": {
        "id": "QzF3VePRLJ22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Number of iterations during training:"
      ],
      "metadata": {
        "id": "UrUbFU-xLRGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net.n_iter_"
      ],
      "metadata": {
        "id": "oxqrQtT0LPbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the dataset, scaler and the network into files."
      ],
      "metadata": {
        "id": "aR5F17nKLlLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "joblib.dump(ds,'mlp_dataset.pkl')\n",
        "joblib.dump(scaler, 'mlp_scaler.pkl') \n",
        "joblib.dump(net, 'mlp_net.pkl')"
      ],
      "metadata": {
        "id": "szgOfPuALm_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the files (tested in Google Chrome, won't work with Firefox)."
      ],
      "metadata": {
        "id": "fvq2kG9ILrY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('mlp_dataset.pkl')\n",
        "files.download('mlp_scaler.pkl')\n",
        "files.download('mlp_net.pkl')"
      ],
      "metadata": {
        "id": "9XkQu3MTLx09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Can the network do better?\n",
        "\n",
        "Re-train the network several times with the same dataset, and check what is the highest value for the percentage of correct classification. You may increase the number of hidden neurons, or even use two hidden layers.\n",
        "\n",
        "## Using a larger dataset\n",
        "\n",
        "Now increase the size of the dataset from 250 to 2500 and 25000 samples, and repeat the process. What happened?\n",
        "\n",
        "## File check\n",
        "You can save the files of your dataset, scaler and network, and check them with [this notebook](https://github.com/RobInLabUJI/MultilayerPerceptron/blob/main/Classification_check.ipynb).\n",
        "\n",
        "## Workshop\n",
        "\n",
        "**REMEMBER**: for the **workshop** of sessions 2-4 you will submit the <tt>pkl</tt> files for the dataset, scaler and neural network that solves the classification example."
      ],
      "metadata": {
        "id": "UhyPy3OBLTbI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FCCcZlxaNTP3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}