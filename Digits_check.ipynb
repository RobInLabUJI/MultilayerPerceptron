{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOUtnURtRYalQ2dU5IYdYV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RobInLabUJI/MultilayerPerceptron/blob/main/Digits_check.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recognizing hand-written digits: checking the network"
      ],
      "metadata": {
        "id": "DKwG5eU1OvGz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixoVxj4EOtIF"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets, metrics\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The digits dataset\n",
        "\n",
        "The data that we are interested in is a set of 1797 images of digits, each one made of $8\\times8$ pixels."
      ],
      "metadata": {
        "id": "-yIwiDptO9J0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "digits = datasets.load_digits()"
      ],
      "metadata": {
        "id": "yuLsF8hCO7jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To apply a classifier on this data, we need to flatten the image, to\n",
        "turn the data in a (samples, feature) matrix:"
      ],
      "metadata": {
        "id": "Mws6TGcUPECM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_samples = len(digits.images)\n",
        "data = digits.images.reshape((n_samples, -1))"
      ],
      "metadata": {
        "id": "OOFRgrz5PCfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cross-validation: evaluating network performance\n",
        "\n",
        "Learning the parameters of the MLP and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called [**overfitting**](https://en.wikipedia.org/wiki/Overfitting). To avoid it, it is common practice  to hold out part of the available data as a test set `[X_test, y_test]`. \n",
        "\n",
        "In scikit-learn a random split into training and test sets can be quickly computed with the [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split) helper function."
      ],
      "metadata": {
        "id": "HgxowhRKPOIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = data, digits.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)"
      ],
      "metadata": {
        "id": "AQNRm519PLgP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload files"
      ],
      "metadata": {
        "id": "vDh7t2WnRmeI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "amX1-r11Rojy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data scaling\n",
        "\n",
        "Standardization of datasets is a common requirement for neural networks; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance.\n",
        "\n",
        "In scikit-learn, the preprocessing module provides a utility class [StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) that computes the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set."
      ],
      "metadata": {
        "id": "aZsNlNkGPVYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Don't cheat - load the scaler from file\n",
        "scaler = joblib.load('digits_scaler.pkl') \n",
        "X_train = scaler.transform(X_train)\n",
        "# apply same transformation to test data\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "zgPtdQq-PTVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the network"
      ],
      "metadata": {
        "id": "9DF7xI2BPYqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = joblib.load('digits_net.pkl')"
      ],
      "metadata": {
        "id": "3Ufv0nFHPXIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis of the network\n",
        "\n",
        "In scikit-learn there are some functions for measuring the accuracy of the classification.\n",
        "\n",
        "### Classification report\n",
        "\n",
        "The [classification_report](http://scikit-learn.org/stable/modules/model_evaluation.html#classification-report) function builds a text report showing the main classification metrics:\n",
        "\n",
        "* [precision](https://en.wikipedia.org/wiki/Precision_and_recall): ratio $\\frac{tp}{tp + fp}$ where $tp$ is the number of true positives and $fp$ the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n",
        "* [recall](https://en.wikipedia.org/wiki/Precision_and_recall): ratio $\\frac{tp}{tp + fn}$ where $tp$ is the number of true positives and $fn$ the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.\n",
        "* [f1-score](https://en.wikipedia.org/wiki/F1_score): a weighted average of the precision and recall, where an $F_1$ score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the $F_1$ score are equal. The formula for the $F_1$ score is: $F_1 = 2\\cdot\\frac{precision \\cdot recall}{precision + recall}$\n",
        "* support: the support is the number of occurrences of each class"
      ],
      "metadata": {
        "id": "ZxSM6qnLP3tR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "expected = y_test\n",
        "predicted = net.predict(X_test)\n",
        "print(metrics.classification_report(expected, predicted))"
      ],
      "metadata": {
        "id": "wvFbCAnQP3PW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion matrix\n",
        "\n",
        "The [`confusion_matrix`](http://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix) function evaluates classification accuracy by computing the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).\n",
        "\n",
        "By definition, a confusion matrix $C$ is such that $C_{i, j}$ is equal to the number of observations known to be in group $i$ but predicted to be in group $j$. Thus an optimal classification result would produce a diagonal matrix."
      ],
      "metadata": {
        "id": "5FVWIspsP9K6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(metrics.confusion_matrix(expected, predicted))"
      ],
      "metadata": {
        "id": "_AtK0-qZP8Yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training curve\n",
        "\n",
        "Number of iterations during training:"
      ],
      "metadata": {
        "id": "QtN9s2tpQCHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net.n_iter_"
      ],
      "metadata": {
        "id": "dc7a_K4RQARG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss curve: (currently, [MLPClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier) supports only the [Cross-Entropy loss function](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression))"
      ],
      "metadata": {
        "id": "PVfKyugbQFXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(net.loss_curve_);\n",
        "plt.xlabel('Iterations');\n",
        "plt.ylabel('Loss');"
      ],
      "metadata": {
        "id": "4iRUG6ECQEgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WsSHKDtATpKe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}